{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Victor's Code Notes\n\n\nThis is a small collection of code notes that I'm gathering after finally ditching OneNote.\n\n\nFeel free to clone them \nhere\n.\n\n\nAbout the autor\n\n\nVictor Ferrer is a Software Engineer, currently working at Ericsson.\n\n\nContact\n\n\n\n\nBlog\n\n\nStack Overflow profile\n\n\nLinkedin Profile",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-victors-code-notes",
            "text": "This is a small collection of code notes that I'm gathering after finally ditching OneNote.  Feel free to clone them  here .  About the autor  Victor Ferrer is a Software Engineer, currently working at Ericsson.  Contact   Blog  Stack Overflow profile  Linkedin Profile",
            "title": "Welcome to Victor's Code Notes"
        },
        {
            "location": "/bash/",
            "text": "Iterate over files and search for a pattern\n\n\nThis script iterates over all XML files in the current directory, searches for the pattern \n\"Name=\"\n and extracts the XML value. Fnally renames the original XML file with the recovered value.\n\n\nScript\n\n\n#!/bin/bash\n\nfor filename in ./*.xml; do\n    echo Checking $filename\n    aux1=`grep -oE ' Name=\\\"(.*)\\\"' < $filename | cut -f 2 -d '\"'`\n    cp $filename $aux1.xml\ndone\n\necho Done!\n\n\n\n\nRemarks\n\n\n\n\ngrep -oE\n searches ONLY for a pattern\n\n\ngrep -oE < $fileName\n is more efficient than \ncat $filename | grep -oE\n\n\nSee \nthis StackOverflow question",
            "title": "Bash"
        },
        {
            "location": "/bash/#iterate-over-files-and-search-for-a-pattern",
            "text": "This script iterates over all XML files in the current directory, searches for the pattern  \"Name=\"  and extracts the XML value. Fnally renames the original XML file with the recovered value.  Script  #!/bin/bash\n\nfor filename in ./*.xml; do\n    echo Checking $filename\n    aux1=`grep -oE ' Name=\\\"(.*)\\\"' < $filename | cut -f 2 -d '\"'`\n    cp $filename $aux1.xml\ndone\n\necho Done!  Remarks   grep -oE  searches ONLY for a pattern  grep -oE < $fileName  is more efficient than  cat $filename | grep -oE  See  this StackOverflow question",
            "title": "Iterate over files and search for a pattern"
        },
        {
            "location": "/bigdata/hive/",
            "text": "Hive DDL Manual\n\n\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL\n\n\nHive tables over Snappy Parquet\n\n\nUseful script for creating partitioned, snappy-based, Hive tables.\n\n\ndrop table mytable;\n\nCREATE EXTERNAL TABLE `mytable`(\n      `mytimestamp` string, \n      `key` string,       \n      `value` double, \n    PARTITIONED BY ( \n      `year` int, \n      `month` int, \n      `day` int)\n  STORED AS PARQUET TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n\nalter table mytable set location 'hdfs://[HDFS_HOST]:8020/user/victor/mytable';\n\nmsck repair table mytable;\n\nselect * from mytable limit 100;\n\n\n\n\nNotes\n\n\n\n\nData should be stored in Snappy Parquet files and respecting the defined partitioning\n\n\nSee \nthis example\n, where a DataFrame writes compressed data.",
            "title": "Hive"
        },
        {
            "location": "/bigdata/hive/#hive-ddl-manual",
            "text": "https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL",
            "title": "Hive DDL Manual"
        },
        {
            "location": "/bigdata/hive/#hive-tables-over-snappy-parquet",
            "text": "Useful script for creating partitioned, snappy-based, Hive tables.  drop table mytable;\n\nCREATE EXTERNAL TABLE `mytable`(\n      `mytimestamp` string, \n      `key` string,       \n      `value` double, \n    PARTITIONED BY ( \n      `year` int, \n      `month` int, \n      `day` int)\n  STORED AS PARQUET TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\");\n\nalter table mytable set location 'hdfs://[HDFS_HOST]:8020/user/victor/mytable';\n\nmsck repair table mytable;\n\nselect * from mytable limit 100;  Notes   Data should be stored in Snappy Parquet files and respecting the defined partitioning  See  this example , where a DataFrame writes compressed data.",
            "title": "Hive tables over Snappy Parquet"
        },
        {
            "location": "/bigdata/parquet/",
            "text": "Manual Parquet generation with Kite SDK\n\n\nInstallation:\n\n\ncurl http://central.maven.org/maven2/org/kitesdk/kite-tools/1.1.0/kite-tools-1.1.0-binary.jar -o kite-dataset\nchmod +x kite-dataset\n\n\n\n\nCreate the files\u00b4 schema:\n\n\n{\n  \"type\" : \"record\",\n  \"name\" : \"GenericCell4gHdfs\",\n  \"namespace\" : \"com.mydomain.mypojo\",\n  \"doc\" : \"Sample object\",\n  \"fields\" : [ {\n    \"name\" : \"timestamp\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"value\",\n    \"type\" : [ \"null\", \"double\" ],\n    \"default\" : null\n  }, {\n    \"name\" : \"longValue\",\n    \"type\" : \"long\"\n  }, {\n    \"name\" : \"intValue\",\n    \"type\" : \"int\"\n  } ]\n}\n\n\n\n\n\nCreate the partitions`definition:\n\n\nWith this JSON:\n\n\n[ {\n  \"name\" : \"year\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"year\"\n}, {\n  \"name\" : \"month\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"month\"\n}, {\n  \"name\" : \"day\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"day\"\n} ]\n\n\n\n\nAnd then invoke:\n\n\nkite-dataset  partition-config timestamp:year timestamp:month timestamp:day -s schema.txt -o partitions.txt\n\n\n\n\nCreate the dataset\n\n\n kite-dataset create dataset:hdfs:/victor/mypojo --schema edos_schema.txt  --format parquet -p partitions.txt\n\n\n\n\nFinally, Import the data from a CSV\n\n\nkite-dataset csv-import bigdata.csv dataset:hdfs:/victor/mypojo --use-hdfs",
            "title": "Parquet"
        },
        {
            "location": "/bigdata/parquet/#manual-parquet-generation-with-kite-sdk",
            "text": "Installation:  curl http://central.maven.org/maven2/org/kitesdk/kite-tools/1.1.0/kite-tools-1.1.0-binary.jar -o kite-dataset\nchmod +x kite-dataset  Create the files\u00b4 schema:  {\n  \"type\" : \"record\",\n  \"name\" : \"GenericCell4gHdfs\",\n  \"namespace\" : \"com.mydomain.mypojo\",\n  \"doc\" : \"Sample object\",\n  \"fields\" : [ {\n    \"name\" : \"timestamp\",\n    \"type\" : \"string\"\n  }, {\n    \"name\" : \"value\",\n    \"type\" : [ \"null\", \"double\" ],\n    \"default\" : null\n  }, {\n    \"name\" : \"longValue\",\n    \"type\" : \"long\"\n  }, {\n    \"name\" : \"intValue\",\n    \"type\" : \"int\"\n  } ]\n}  Create the partitions`definition:  With this JSON:  [ {\n  \"name\" : \"year\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"year\"\n}, {\n  \"name\" : \"month\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"month\"\n}, {\n  \"name\" : \"day\",\n  \"source\" : \"timestamp\",\n  \"type\" : \"day\"\n} ]  And then invoke:  kite-dataset  partition-config timestamp:year timestamp:month timestamp:day -s schema.txt -o partitions.txt  Create the dataset   kite-dataset create dataset:hdfs:/victor/mypojo --schema edos_schema.txt  --format parquet -p partitions.txt  Finally, Import the data from a CSV  kite-dataset csv-import bigdata.csv dataset:hdfs:/victor/mypojo --use-hdfs",
            "title": "Manual Parquet generation with Kite SDK"
        },
        {
            "location": "/docker/cloudera/",
            "text": "Cloudera Quickstart docker container\n\n\nPull the image\n\n\n# Pulls the image (4Gb!)\ndocker pull cloudera/quickstart\n\n\n\n\nSet a variable with all needed ports to be opened\n\n\n# Ports to be opened (most commonly used)\n# - 8888      expose hue interface\n# - 7180      expose cloudera manager\n# - 80        expose cloudera examples\n# - 8983      expose port of Web UI solr search \n# - 50070     expose name node web ui interface\n# - 50090     expose secondary name node\n# - 50075     expose data  node\n# - 50030     expose job tracker\n# - 50060     expose task trackers\n# - 60010     expose hbase master status\n# - 60030     expose hbase region server\n# - 9095      expose hbase thrift server\n# - 8020      expose hdfs port\n# - 8088      expose job tracker port\n# - 4040      expose port of spark\n# - 18088     expose history server web interface\n# ...\nports=\"-p 8888:8888 -p 7180:7180 -p 80:80 -p 4040:4040 -p 4041:4041 \\\n-p 4042:4042  -p 4043:4043 -p 9092:9092 -p 2181:2181 -p 8020:8020   \\ \n-p 18088:18088 -p 10000:10000 -p 21050:21050 -p 50070:50070  -p 50075:50075 \\\n-p 50060:50060  -p 50030:50030 -p 50010:50010\"\n\n\n\n\nSet a variable with a volume for file exchange with the container\n\n\n# Volume used to exchange stuff with the running container\nlocalVolumeDir=/home/me/cloudera_exchange_dir\n\n\n\n\nRun the container and store its ID\n\n\ncontainerId=`docker run --hostname=quickstart.cloudera -d \\\n                -v $localVolumeDir:/volume\n                --privileged=true -t -i $ports cloudera/quickstart /usr/bin/docker-quickstart`\n\n\n\n\nInstall and run the Kafka server\n\n\nKafka does not seem to be installed, but you can quickly do it:\n\n\n# See https://kafka.apache.org/quickstart\n\n# Download the binaries and execute this command:\n# > bin/kafka-server-start.sh config/server.properties\n\n# If you want to access Kafka from outside the container, you need to change this line in server.properties\n# zookeeper.connect=localhost:2181\n# And replace it with the Docker host public IP address\n\n\n\n\nOptional: Run Cloudera Manager\n\n\nsudo su\ncd /home/cloudera/\n./cloudera-manager",
            "title": "Cloudera Quickstart"
        },
        {
            "location": "/docker/cloudera/#cloudera-quickstart-docker-container",
            "text": "Pull the image  # Pulls the image (4Gb!)\ndocker pull cloudera/quickstart  Set a variable with all needed ports to be opened  # Ports to be opened (most commonly used)\n# - 8888      expose hue interface\n# - 7180      expose cloudera manager\n# - 80        expose cloudera examples\n# - 8983      expose port of Web UI solr search \n# - 50070     expose name node web ui interface\n# - 50090     expose secondary name node\n# - 50075     expose data  node\n# - 50030     expose job tracker\n# - 50060     expose task trackers\n# - 60010     expose hbase master status\n# - 60030     expose hbase region server\n# - 9095      expose hbase thrift server\n# - 8020      expose hdfs port\n# - 8088      expose job tracker port\n# - 4040      expose port of spark\n# - 18088     expose history server web interface\n# ...\nports=\"-p 8888:8888 -p 7180:7180 -p 80:80 -p 4040:4040 -p 4041:4041 \\\n-p 4042:4042  -p 4043:4043 -p 9092:9092 -p 2181:2181 -p 8020:8020   \\ \n-p 18088:18088 -p 10000:10000 -p 21050:21050 -p 50070:50070  -p 50075:50075 \\\n-p 50060:50060  -p 50030:50030 -p 50010:50010\"  Set a variable with a volume for file exchange with the container  # Volume used to exchange stuff with the running container\nlocalVolumeDir=/home/me/cloudera_exchange_dir  Run the container and store its ID  containerId=`docker run --hostname=quickstart.cloudera -d \\\n                -v $localVolumeDir:/volume\n                --privileged=true -t -i $ports cloudera/quickstart /usr/bin/docker-quickstart`  Install and run the Kafka server  Kafka does not seem to be installed, but you can quickly do it:  # See https://kafka.apache.org/quickstart\n\n# Download the binaries and execute this command:\n# > bin/kafka-server-start.sh config/server.properties\n\n# If you want to access Kafka from outside the container, you need to change this line in server.properties\n# zookeeper.connect=localhost:2181\n# And replace it with the Docker host public IP address  Optional: Run Cloudera Manager  sudo su\ncd /home/cloudera/\n./cloudera-manager",
            "title": "Cloudera Quickstart docker container"
        },
        {
            "location": "/docker/sqlserver/",
            "text": "SQL Server on Docker\n\n\nGeneral Instructions:\nhttps://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker\n\n\nHow to solve \"2017-GA: The transaction log for database 'master' is full due to 'NOTHING'. #180\" error:\nhttps://github.com/Microsoft/mssql-docker/issues/180#issuecomment-371965952\n\n\nsudo docker run -e 'ACCEPT_EULA=Y' -e 'MSSQL_SA_PASSWORD=<YourStrong!Passw0rd>' \\\n                -p 1401:1433 --name sql1 -v /data/ms-sql-docker-data:/var/opt/mssql \\\n                 -d microsoft/mssql-server-linux:2017-latest",
            "title": "SQL Server"
        },
        {
            "location": "/docker/sqlserver/#sql-server-on-docker",
            "text": "General Instructions:\nhttps://docs.microsoft.com/en-us/sql/linux/quickstart-install-connect-docker  How to solve \"2017-GA: The transaction log for database 'master' is full due to 'NOTHING'. #180\" error:\nhttps://github.com/Microsoft/mssql-docker/issues/180#issuecomment-371965952  sudo docker run -e 'ACCEPT_EULA=Y' -e 'MSSQL_SA_PASSWORD=<YourStrong!Passw0rd>' \\\n                -p 1401:1433 --name sql1 -v /data/ms-sql-docker-data:/var/opt/mssql \\\n                 -d microsoft/mssql-server-linux:2017-latest",
            "title": "SQL Server on Docker"
        },
        {
            "location": "/git/ssh-clone/",
            "text": "Clone a repo using a SSH private key\n\n\nTo clone a repo using a previously registred SSH key\n\n\nssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'\n\n\n\n\nSee this \nStackOverflow question",
            "title": "SSH Usage"
        },
        {
            "location": "/git/ssh-clone/#clone-a-repo-using-a-ssh-private-key",
            "text": "To clone a repo using a previously registred SSH key  ssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git'  See this  StackOverflow question",
            "title": "Clone a repo using a SSH private key"
        },
        {
            "location": "/java/crypto/",
            "text": "Use the cryptographic extensions in Java\n\n\nInstallation\n\n\nDownload from \nhere\n and copy to the local Java home folder.\n\n\nCreate a certificate\n\n\nThis is an easy way to generate a self-signed certificate. Not a recommended production setup!\n\n\n$ keytool -genkeypair -alias mytestkey -keyalg RSA \\ \n          -dname \"CN=WebServer,OU=Unit,O=Organization,L=City,S=State,C=US\" \\ \n          -keypass changeme -keystore server.jks -storepass letmein",
            "title": "Cryptography"
        },
        {
            "location": "/java/crypto/#use-the-cryptographic-extensions-in-java",
            "text": "Installation  Download from  here  and copy to the local Java home folder.  Create a certificate  This is an easy way to generate a self-signed certificate. Not a recommended production setup!  $ keytool -genkeypair -alias mytestkey -keyalg RSA \\ \n          -dname \"CN=WebServer,OU=Unit,O=Organization,L=City,S=State,C=US\" \\ \n          -keypass changeme -keystore server.jks -storepass letmein",
            "title": "Use the cryptographic extensions in Java"
        },
        {
            "location": "/java/jvm/",
            "text": "How to open JMX ports\n\n\nJVM options to be set:\n\n\n-Djavax.management.builder.initial= \n-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.port=8855 \n-Dcom.sun.management.jmxremote.authenticate=false \n-Dcom.sun.management.jmxremote.ssl=false\n\n\n\n\nSee \nthis\n\n\nHow to set up JVM memory settings\n\n\nJVM options to be set:\n\n\n# Starts with 16G and set a maximum of 24G\n-Xms16G -Xmx24G",
            "title": "JVM"
        },
        {
            "location": "/java/jvm/#how-to-open-jmx-ports",
            "text": "JVM options to be set:  -Djavax.management.builder.initial= \n-Dcom.sun.management.jmxremote \n-Dcom.sun.management.jmxremote.port=8855 \n-Dcom.sun.management.jmxremote.authenticate=false \n-Dcom.sun.management.jmxremote.ssl=false  See  this",
            "title": "How to open JMX ports"
        },
        {
            "location": "/java/jvm/#how-to-set-up-jvm-memory-settings",
            "text": "JVM options to be set:  # Starts with 16G and set a maximum of 24G\n-Xms16G -Xmx24G",
            "title": "How to set up JVM memory settings"
        },
        {
            "location": "/java/properties/",
            "text": "Load a java.util.properties from File/URL/HDFS\n\n\n/**\n* Utility class that provides logic for reading Properties objects from several locations:\n* - URL (HTTP/S)\n* - File\n* - HDFS \n*/\npublic class PropertiesReader {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(PropertiesReader.class);\n\n    private static final String HTTP_RESOURCE_REGEX = \"(http[s]?://.*[:[0-9]+]?/.*)\";\n\n    /**\n     * Gets an InputStream from either a File, URL or HDFS\n    public static InputStream getFileInputStream(String path) throws IOException \n    {\n        // Try with an HTTP(S) resource\n        Pattern httpPat = Pattern.compile(HTTP_RESOURCE_REGEX);\n        Matcher httpMatcher = httpPat.matcher(path);\n\n        if (httpMatcher.matches()){\n            return getHTTPInputStream(httpMatcher.group());\n        }\n\n        // TODO Copy HDFS block\n\n        // Now try with a regular file\n        return new FileInputStream(path);\n    }\n\n    /**\n     * @param path String representing the location of the properties file. Can be HDFS, URL or local file\n     *  @return the loaded Properties object\n     *  @throws URISyntaxException \n     * @throws IOException \n     */\n    public static Properties getProperties(String path) throws IOException\n    {\n        // Try with an HDFS resource\n        Properties toReturn = new Properties();\n        toReturn.load(getFileInputStream(path));\n\n        return toReturn;\n    }\n\n    private static InputStream getHTTPInputStream(String path) throws IOException {\n        LOGGER.info(String.format(\"Trying to open URL at \" + path));\n        return new URL(path);\n    }\n}",
            "title": "File/URL/HDFS Properties"
        },
        {
            "location": "/java/properties/#load-a-javautilproperties-from-fileurlhdfs",
            "text": "/**\n* Utility class that provides logic for reading Properties objects from several locations:\n* - URL (HTTP/S)\n* - File\n* - HDFS \n*/\npublic class PropertiesReader {\n\n    private static final Logger LOGGER = LoggerFactory.getLogger(PropertiesReader.class);\n\n    private static final String HTTP_RESOURCE_REGEX = \"(http[s]?://.*[:[0-9]+]?/.*)\";\n\n    /**\n     * Gets an InputStream from either a File, URL or HDFS\n    public static InputStream getFileInputStream(String path) throws IOException \n    {\n        // Try with an HTTP(S) resource\n        Pattern httpPat = Pattern.compile(HTTP_RESOURCE_REGEX);\n        Matcher httpMatcher = httpPat.matcher(path);\n\n        if (httpMatcher.matches()){\n            return getHTTPInputStream(httpMatcher.group());\n        }\n\n        // TODO Copy HDFS block\n\n        // Now try with a regular file\n        return new FileInputStream(path);\n    }\n\n    /**\n     * @param path String representing the location of the properties file. Can be HDFS, URL or local file\n     *  @return the loaded Properties object\n     *  @throws URISyntaxException \n     * @throws IOException \n     */\n    public static Properties getProperties(String path) throws IOException\n    {\n        // Try with an HDFS resource\n        Properties toReturn = new Properties();\n        toReturn.load(getFileInputStream(path));\n\n        return toReturn;\n    }\n\n    private static InputStream getHTTPInputStream(String path) throws IOException {\n        LOGGER.info(String.format(\"Trying to open URL at \" + path));\n        return new URL(path);\n    }\n}",
            "title": "Load a java.util.properties from File/URL/HDFS"
        },
        {
            "location": "/scala/cheatsheets/",
            "text": "Scala cheatsheets and job questions\n\n\n\n\nJournal Dev Scala Interview Questions\n\n\nScala Interview Questions - Github",
            "title": "Cheatsheets"
        },
        {
            "location": "/scala/cheatsheets/#scala-cheatsheets-and-job-questions",
            "text": "Journal Dev Scala Interview Questions  Scala Interview Questions - Github",
            "title": "Scala cheatsheets and job questions"
        },
        {
            "location": "/scala/json/",
            "text": "Code for parsing a JSON document\n\n\nThese auxiliary classes help retrieving the values of the different types of JSON objects:\n\n\n// This common class is the base of all classes parsed from the JSON\nclass CC[T] { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) }\n\n// Now, for every type that we might recover, we have a specialized class\n// M (map) and L (list) will be used to recover embedded objects from the JSON\nobject M extends CC[Map[String, Any]]\nobject L extends CC[List[Any]]\nobject S extends CC[String]\nobject D extends CC[Double]\nobject B extends CC[Boolean]\n\n\n\n\n\nNow, after parsing the JSON structure, we crawl along it and extract the needed fields\n\n\ndef parseJSONData(data: String) : List[UseCase] = {\n\n// We crawl along the JSON structure, retrieving the columns that will form our table structure\nval resultList = for {\n  Some(M(json)) <- List(JSON.parseFull(data))\n  // TODO\n\n} yield {\n  // TODO\n}",
            "title": "Parse a JSON"
        },
        {
            "location": "/scala/json/#code-for-parsing-a-json-document",
            "text": "These auxiliary classes help retrieving the values of the different types of JSON objects:  // This common class is the base of all classes parsed from the JSON\nclass CC[T] { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) }\n\n// Now, for every type that we might recover, we have a specialized class\n// M (map) and L (list) will be used to recover embedded objects from the JSON\nobject M extends CC[Map[String, Any]]\nobject L extends CC[List[Any]]\nobject S extends CC[String]\nobject D extends CC[Double]\nobject B extends CC[Boolean]  Now, after parsing the JSON structure, we crawl along it and extract the needed fields  def parseJSONData(data: String) : List[UseCase] = {\n\n// We crawl along the JSON structure, retrieving the columns that will form our table structure\nval resultList = for {\n  Some(M(json)) <- List(JSON.parseFull(data))\n  // TODO\n\n} yield {\n  // TODO\n}",
            "title": "Code for parsing a JSON document"
        },
        {
            "location": "/spring/cloud/",
            "text": "Add encryption to a Spring Cloud Config\n\n\nSee \nhere\n on how to setup a certificate. This section describes how to activate encryption in a Spring Cloud Config server.\n\n\nNew properties to use\n\n\n# This can be an external location instead!\n# Given by an administration during deployment time\nencrypt.keyStore.location=classpath:/server.jks\n# These properties can be provided as environment variables (more secure!)\nencrypt.keyStore.password=letmein\nencrypt.keyStore.alias=mytestkey\nencrypt.keyStore.secret=changeme\n\n\n\n\nEncryption endpoint for administrator usage\n\n\nThis endpoints allows encryption/decryption of single properties:\n\n\n# Decryption\ncurl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/decrypt -d [ENCRYPTED_TEXT]\n\n# Encryption\ncurl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/encrypt -d myPassword",
            "title": "Cloud"
        },
        {
            "location": "/spring/cloud/#add-encryption-to-a-spring-cloud-config",
            "text": "See  here  on how to setup a certificate. This section describes how to activate encryption in a Spring Cloud Config server.  New properties to use  # This can be an external location instead!\n# Given by an administration during deployment time\nencrypt.keyStore.location=classpath:/server.jks\n# These properties can be provided as environment variables (more secure!)\nencrypt.keyStore.password=letmein\nencrypt.keyStore.alias=mytestkey\nencrypt.keyStore.secret=changeme  Encryption endpoint for administrator usage  This endpoints allows encryption/decryption of single properties:  # Decryption\ncurl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/decrypt -d [ENCRYPTED_TEXT]\n\n# Encryption\ncurl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/encrypt -d myPassword",
            "title": "Add encryption to a Spring Cloud Config"
        },
        {
            "location": "/spring/core/",
            "text": "Use two different DataSources\n\n\nTo use more than the predefined DataSource in Spring, for instance an H2 Database:\n\n\n### Add this new set of properties ###\n\n## URL used to connect to the H2 Database\nspring.secondDatasource.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE\n## Driver class used to connect to the H2 database (it will depend on datasource).\nspring.secondDatasource.driver-class-Name=org.h2.Driver\n## User name \nspring.secondDatasource.username=xxx\n## Password \nspring.secondDatasource.password=xxx\n## Datasource configuration for jobs database.\nspring.jpa.hibernate.ddl-auto=create-drop\nspring.secondDatasource.initialize=true\nspring.secondDatasource.test-on-borrow=true\nspring.secondDatasource.validation-query=select 1\n\n\n\n\n\nAdd this @Configuration class:\n\n\n/**\n * Config class holding several datasources, the default one and the H2\n */\n@Configuration\npublic class DataSourceConfiguration\n{\n    @Bean\n    @Qualifier(\"businessDataSource\")\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    public DataSource primaryDataSource()\n    {\n        return DataSourceBuilder.create().build();\n    }\n\n    @Bean\n    @Primary\n    @Qualifier(\"jobsDataSource\")\n    @ConfigurationProperties(prefix = \"spring.secondDatasource\")\n    public DataSource secondaryDataSource()\n    {\n        return DataSourceBuilder.create().build();\n    }\n}",
            "title": "Core"
        },
        {
            "location": "/spring/core/#use-two-different-datasources",
            "text": "To use more than the predefined DataSource in Spring, for instance an H2 Database:  ### Add this new set of properties ###\n\n## URL used to connect to the H2 Database\nspring.secondDatasource.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE\n## Driver class used to connect to the H2 database (it will depend on datasource).\nspring.secondDatasource.driver-class-Name=org.h2.Driver\n## User name \nspring.secondDatasource.username=xxx\n## Password \nspring.secondDatasource.password=xxx\n## Datasource configuration for jobs database.\nspring.jpa.hibernate.ddl-auto=create-drop\nspring.secondDatasource.initialize=true\nspring.secondDatasource.test-on-borrow=true\nspring.secondDatasource.validation-query=select 1  Add this @Configuration class:  /**\n * Config class holding several datasources, the default one and the H2\n */\n@Configuration\npublic class DataSourceConfiguration\n{\n    @Bean\n    @Qualifier(\"businessDataSource\")\n    @ConfigurationProperties(prefix = \"spring.datasource\")\n    public DataSource primaryDataSource()\n    {\n        return DataSourceBuilder.create().build();\n    }\n\n    @Bean\n    @Primary\n    @Qualifier(\"jobsDataSource\")\n    @ConfigurationProperties(prefix = \"spring.secondDatasource\")\n    public DataSource secondaryDataSource()\n    {\n        return DataSourceBuilder.create().build();\n    }\n}",
            "title": "Use two different DataSources"
        },
        {
            "location": "/spring/batch/",
            "text": "Spring Batch: Use seconday datasource for metadata\n\n\nOn top of \nthis example\n, add this @Configuration class:\n\n\nimport org.springframework.batch.core.configuration.annotation.DefaultBatchConfigurer;\n\n// Another @Configuration class...\n\n@Autowired\n@Qualifier(\"jobsDataSource\")\nprivate DataSource dataSource;\n\n@Bean\npublic BatchConfigurer configurer()\n{\n    // This is required to avoid problems when jobs datasource is into some secondary datasource.\n    return new DefaultBatchConfigurer(dataSource);\n}",
            "title": "Batch"
        },
        {
            "location": "/spring/batch/#spring-batch-use-seconday-datasource-for-metadata",
            "text": "On top of  this example , add this @Configuration class:  import org.springframework.batch.core.configuration.annotation.DefaultBatchConfigurer;\n\n// Another @Configuration class...\n\n@Autowired\n@Qualifier(\"jobsDataSource\")\nprivate DataSource dataSource;\n\n@Bean\npublic BatchConfigurer configurer()\n{\n    // This is required to avoid problems when jobs datasource is into some secondary datasource.\n    return new DefaultBatchConfigurer(dataSource);\n}",
            "title": "Spring Batch: Use seconday datasource for metadata"
        },
        {
            "location": "/spring/integration/",
            "text": "SFTP with Spring Integration DSL\n\n\nTODO",
            "title": "Integration"
        },
        {
            "location": "/spring/integration/#sftp-with-spring-integration-dsl",
            "text": "TODO",
            "title": "SFTP with Spring Integration DSL"
        },
        {
            "location": "/spring/security/",
            "text": "Simple set up with Spring Security and Spring Boot\n\n\nSimple in-memory authentication with special rules for several endopoints (local only)\n\n\n@Configuration\npublic class SecurityConfig extends WebSecurityConfigurerAdapter {\n\n    ...\n\n    @Autowired\n    public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception {\n        auth\n            .inMemoryAuthentication()\n            .withUser(\"user\").password(user_password).roles(\"USER\")\n        .and()\n            .withUser(\"admin\").password(admin_password).authorities(\"ADMIN\");\n    }\n\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n            .csrf()\n            .disable()\n            .httpBasic()\n         .and()\n            .authorizeRequests()\n            .antMatchers(\"/admin/config/decrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\")\n            .antMatchers(\"/admin/config/encrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\")\n            .antMatchers(\"/admin/config/**\").authenticated()\n            .antMatchers(\"/**\").permitAll()\n        ;\n    }\n}\n\n\n\n\nTo use IPV4 instead of IPV6 use this:\n\n\n-Djava.net.preferIPv4Stack=true",
            "title": "Security"
        },
        {
            "location": "/spring/security/#simple-set-up-with-spring-security-and-spring-boot",
            "text": "Simple in-memory authentication with special rules for several endopoints (local only)  @Configuration\npublic class SecurityConfig extends WebSecurityConfigurerAdapter {\n\n    ...\n\n    @Autowired\n    public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception {\n        auth\n            .inMemoryAuthentication()\n            .withUser(\"user\").password(user_password).roles(\"USER\")\n        .and()\n            .withUser(\"admin\").password(admin_password).authorities(\"ADMIN\");\n    }\n\n    @Override\n    protected void configure(HttpSecurity http) throws Exception {\n        http\n            .csrf()\n            .disable()\n            .httpBasic()\n         .and()\n            .authorizeRequests()\n            .antMatchers(\"/admin/config/decrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\")\n            .antMatchers(\"/admin/config/encrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\")\n            .antMatchers(\"/admin/config/**\").authenticated()\n            .antMatchers(\"/**\").permitAll()\n        ;\n    }\n}  To use IPV4 instead of IPV6 use this:  -Djava.net.preferIPv4Stack=true",
            "title": "Simple set up with Spring Security and Spring Boot"
        },
        {
            "location": "/spark/spark/",
            "text": "Read and write from/to CSV\n\n\nIf Spark Version is older than 2.0, this dependency is needed in SBT;\n\n\n\"com.databricks\"%\"spark-csv_2.10\"%\"1.5.0\"\n\n\n\n\nThen load the CSV into a DataFrame:\n\n\n# Need to provide a Schame and a File Path\nval valdf=sqlContext.read\n.format(\"com.databricks.spark.csv\")\n.schema(schema)\n.option(\"delimiter\",\",\")\n.option(\"nullValue\",\"\")\n.option(\"header\",\"true\")\n.option(\"treatEmptyValuesAsNulls\",\"true\")\n.load(csv_path)\n\n\n\n\nTo write the DataFrame to a SINGLE CSV file:\n\n\n// Write all to one partition\ndf.coalesce(1).write.format(\"com.databricks.spark.csv\")\n.option(\"header\",\"true\")\n.save(\"mydata.csv\")\n} \n\n\n\n\nWriting a JSON DataFrame to Kafka\n\n\nWriting a RDD or DataFrame to Kafka requires the Producers to be created in each RDD partition\n\n\n  private def writeJSONToKafka(df : DataFrame, topic: String): Unit =\n  {\n    val props = new util.HashMap[String, Object]()\n    props.put(\"bootstrap.servers\",\"localhost:9092\")\n    props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\")\n    props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\")\n\n    println(\"Writing JSON to Kafka\")\n\n    df.toJSON.foreachPartition((partisions: Iterator[String]) => {\n      val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)\n      partisions.foreach((line: String) => {\n          producer.send(new ProducerRecord[String, String](topic,null,line))\n      })\n    })\n  }\n\n\n\n\n\nSpark Streaming setup\n\n\nSBT dependencies to be set:\n\n\nlibraryDependencies ++= {\n\n  val sparkVer = \"2.0.0\"\n  Seq(\n    \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-streaming-kafka-0-8\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(),\n    \"org.apache.spark\" %% \"spark-hive\" % sparkVer % \"provided\"\n  )\n}\n\n\n\n\nStream set up and processing:\n\n\n   // Initialize spark context\n    val conf = new SparkConf().setAppName(\"My Streaming Processor\")\n    val ssc = new StreamingContext(conf, Seconds(30))\n    ssc.sparkContext.setLogLevel(\"ERROR\")\n\n    // Kafka config\n    val kafkaConf = Map(\n    // TODO Maybe this one is all we need\n    \"metadata.broker.list\" -> \"localhost:9092\",\n    )\n\n    // Create a stream of Array[Byte] as key and as payload\n    val stream = KafkaUtils.createDirectStream[Array[Byte], Array[Byte], DefaultDecoder, DefaultDecoder](\n    ssc,\n    kafkaConf,\n    Set(\"myTopic\")\n    )\n\n    // Process each batch of data in another function\n    stream.foreachRDD { rdd => processRdd(rdd,config)}\n\n    ssc.start()\n    ssc.awaitTermination()\n\n\n\n\nWrite a compressed DataFrame\n\n\n  val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()\n  spark.sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\n\n  // Create the DF and add the partition columns\n  val targetDF = spark.sqlContext.createDataFrame(rowData, addPartitions(myTableSchema,partitionsArray))\n\n  // Write to Parquet\n  val parquetPath = \"/user/victor/mytable\"\n  targetDF.write.mode(\"append\").partitionBy(partitions : _*).parquet(parquetPath)\n\n\n\n\n\nAdd dynamic partitions to a DataFrame\n\n\nAdds dynamic partitions to an existing schema, so they can be given by configuration\n\n\n  def addPartitions(schema: StructType, partitions: Array[String]) : StructType = {\n    var array = new ArrayBuffer[StructField]()\n\n    // Add previous fields\n    schema.foreach(f => array += f)\n\n    // Add new partition fields\n    partitions.foreach(p => array += new StructField(p,StringType))\n\n    new StructType(array.toArray)\n  }\n\n  val myTableSchema = StructType(Array(StructField(\"mytimestamp\", StringType, true),\n    StructField(\"key\", StringType, true),\n    StructField(\"value\", DoubleType, true)))",
            "title": "DataFrames"
        },
        {
            "location": "/spark/spark/#read-and-write-fromto-csv",
            "text": "If Spark Version is older than 2.0, this dependency is needed in SBT;  \"com.databricks\"%\"spark-csv_2.10\"%\"1.5.0\"  Then load the CSV into a DataFrame:  # Need to provide a Schame and a File Path\nval valdf=sqlContext.read\n.format(\"com.databricks.spark.csv\")\n.schema(schema)\n.option(\"delimiter\",\",\")\n.option(\"nullValue\",\"\")\n.option(\"header\",\"true\")\n.option(\"treatEmptyValuesAsNulls\",\"true\")\n.load(csv_path)  To write the DataFrame to a SINGLE CSV file:  // Write all to one partition\ndf.coalesce(1).write.format(\"com.databricks.spark.csv\")\n.option(\"header\",\"true\")\n.save(\"mydata.csv\")\n}",
            "title": "Read and write from/to CSV"
        },
        {
            "location": "/spark/spark/#writing-a-json-dataframe-to-kafka",
            "text": "Writing a RDD or DataFrame to Kafka requires the Producers to be created in each RDD partition    private def writeJSONToKafka(df : DataFrame, topic: String): Unit =\n  {\n    val props = new util.HashMap[String, Object]()\n    props.put(\"bootstrap.servers\",\"localhost:9092\")\n    props.put(\"key.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\")\n    props.put(\"value.serializer\",  \"org.apache.kafka.common.serialization.StringSerializer\")\n\n    println(\"Writing JSON to Kafka\")\n\n    df.toJSON.foreachPartition((partisions: Iterator[String]) => {\n      val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props)\n      partisions.foreach((line: String) => {\n          producer.send(new ProducerRecord[String, String](topic,null,line))\n      })\n    })\n  }",
            "title": "Writing a JSON DataFrame to Kafka"
        },
        {
            "location": "/spark/spark/#spark-streaming-setup",
            "text": "SBT dependencies to be set:  libraryDependencies ++= {\n\n  val sparkVer = \"2.0.0\"\n  Seq(\n    \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-streaming-kafka-0-8\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(),\n    \"org.apache.spark\" %% \"spark-hive\" % sparkVer % \"provided\"\n  )\n}  Stream set up and processing:     // Initialize spark context\n    val conf = new SparkConf().setAppName(\"My Streaming Processor\")\n    val ssc = new StreamingContext(conf, Seconds(30))\n    ssc.sparkContext.setLogLevel(\"ERROR\")\n\n    // Kafka config\n    val kafkaConf = Map(\n    // TODO Maybe this one is all we need\n    \"metadata.broker.list\" -> \"localhost:9092\",\n    )\n\n    // Create a stream of Array[Byte] as key and as payload\n    val stream = KafkaUtils.createDirectStream[Array[Byte], Array[Byte], DefaultDecoder, DefaultDecoder](\n    ssc,\n    kafkaConf,\n    Set(\"myTopic\")\n    )\n\n    // Process each batch of data in another function\n    stream.foreachRDD { rdd => processRdd(rdd,config)}\n\n    ssc.start()\n    ssc.awaitTermination()",
            "title": "Spark Streaming setup"
        },
        {
            "location": "/spark/spark/#write-a-compressed-dataframe",
            "text": "val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate()\n  spark.sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\")\n\n  // Create the DF and add the partition columns\n  val targetDF = spark.sqlContext.createDataFrame(rowData, addPartitions(myTableSchema,partitionsArray))\n\n  // Write to Parquet\n  val parquetPath = \"/user/victor/mytable\"\n  targetDF.write.mode(\"append\").partitionBy(partitions : _*).parquet(parquetPath)",
            "title": "Write a compressed DataFrame"
        },
        {
            "location": "/spark/spark/#add-dynamic-partitions-to-a-dataframe",
            "text": "Adds dynamic partitions to an existing schema, so they can be given by configuration    def addPartitions(schema: StructType, partitions: Array[String]) : StructType = {\n    var array = new ArrayBuffer[StructField]()\n\n    // Add previous fields\n    schema.foreach(f => array += f)\n\n    // Add new partition fields\n    partitions.foreach(p => array += new StructField(p,StringType))\n\n    new StructType(array.toArray)\n  }\n\n  val myTableSchema = StructType(Array(StructField(\"mytimestamp\", StringType, true),\n    StructField(\"key\", StringType, true),\n    StructField(\"value\", DoubleType, true)))",
            "title": "Add dynamic partitions to a DataFrame"
        },
        {
            "location": "/spark/structured_streaming/",
            "text": "Structured Streaming Example\n\n\nProducer process used for testing\n\n\nnetstat -c | grep tcp | kafka-console-producer --topic topic2345 --broker-list [KAFKA HOST]:9092\n\n\n\n\nLibraries and command line hacks used:\n\n\nexport SPARK_KAFKA_VERSION=0.10\n\n\n\n\n(see \nthis\n)\n\n\nSBT dependencies\n\n\nlibraryDependencies ++= {\n\n  val sparkVer = \"2.1.0\"\n  Seq(\n    \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(),\n    \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-sql\" % sparkVer % \"provided\",\n    \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % sparkVer,\n    \"org.apache.kafka\" % \"kafka-clients\" % \"0.10.0.1\"\n  )\n}\n\n\n\n\nSpark Submit command\n\n\nexport SPARK_KAFKA_VERSION=0.10 && spark2-submit  --class com.ericsson.streaming.structured.StructuredStreamingMain --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.kafka:kafka-clients:0.10.0.1 sparkstructuredstreamingconsumer_2.11-0.1.jar topic2345 172.29.45.149:9092 WARN\n\n\n\n\nGitHub Book on Spark Structured Streaming",
            "title": "Structured Streaming"
        },
        {
            "location": "/spark/structured_streaming/#structured-streaming-example",
            "text": "Producer process used for testing  netstat -c | grep tcp | kafka-console-producer --topic topic2345 --broker-list [KAFKA HOST]:9092  Libraries and command line hacks used:  export SPARK_KAFKA_VERSION=0.10  (see  this )  SBT dependencies  libraryDependencies ++= {\n\n  val sparkVer = \"2.1.0\"\n  Seq(\n    \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(),\n    \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\",\n    \"org.apache.spark\" %% \"spark-sql\" % sparkVer % \"provided\",\n    \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % sparkVer,\n    \"org.apache.kafka\" % \"kafka-clients\" % \"0.10.0.1\"\n  )\n}  Spark Submit command  export SPARK_KAFKA_VERSION=0.10 && spark2-submit  --class com.ericsson.streaming.structured.StructuredStreamingMain --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.kafka:kafka-clients:0.10.0.1 sparkstructuredstreamingconsumer_2.11-0.1.jar topic2345 172.29.45.149:9092 WARN  GitHub Book on Spark Structured Streaming",
            "title": "Structured Streaming Example"
        }
    ]
}