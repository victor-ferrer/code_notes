{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Victor's Code Notes This is a small collection of code notes that I'm gathering after finally ditching OneNote. Feel free to clone them here . About the autor Victor Ferrer is a Software Engineer, currently working at Ericsson. Contact Blog Stack Overflow profile Linkedin Profile","title":"Home"},{"location":"#welcome-to-victors-code-notes","text":"This is a small collection of code notes that I'm gathering after finally ditching OneNote. Feel free to clone them here .","title":"Welcome to Victor's Code Notes"},{"location":"#about-the-autor","text":"Victor Ferrer is a Software Engineer, currently working at Ericsson.","title":"About the autor"},{"location":"#contact","text":"Blog Stack Overflow profile Linkedin Profile","title":"Contact"},{"location":"bash/","text":"Iterate over files and search for a pattern This script iterates over all XML files in the current directory, searches for the pattern \"Name=\" and extracts the XML value. Fnally renames the original XML file with the recovered value. Script #!/bin/bash for filename in ./*.xml; do echo Checking $filename aux1=`grep -oE ' Name=\\\"(.*)\\\"' < $filename | cut -f 2 -d '\"'` cp $filename $aux1.xml done echo Done! Remarks grep -oE searches ONLY for a pattern grep -oE < $fileName is more efficient than cat $filename | grep -oE See this StackOverflow question # Perform a HTTP request with CURL and extract the status code ``` URL=http://www.google.es HTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" $URL) if [ $HTTP_CODE -eq 200 ] ... ```","title":"Bash"},{"location":"bash/#iterate-over-files-and-search-for-a-pattern","text":"This script iterates over all XML files in the current directory, searches for the pattern \"Name=\" and extracts the XML value. Fnally renames the original XML file with the recovered value. Script #!/bin/bash for filename in ./*.xml; do echo Checking $filename aux1=`grep -oE ' Name=\\\"(.*)\\\"' < $filename | cut -f 2 -d '\"'` cp $filename $aux1.xml done echo Done! Remarks grep -oE searches ONLY for a pattern grep -oE < $fileName is more efficient than cat $filename | grep -oE See this StackOverflow question # Perform a HTTP request with CURL and extract the status code ``` URL=http://www.google.es HTTP_CODE=$(curl -s -o /dev/null -w \"%{http_code}\" $URL) if [ $HTTP_CODE -eq 200 ] ... ```","title":"Iterate over files and search for a pattern"},{"location":"bigdata/hive/","text":"Hive DDL Manual Link Hive tables over Snappy Parquet Useful script for creating partitioned, snappy-based, Hive tables. drop table mytable; CREATE EXTERNAL TABLE `mytable`( `mytimestamp` string, `key` string, `value` double, PARTITIONED BY ( `year` int, `month` int, `day` int) STORED AS PARQUET TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\"); alter table mytable set location 'hdfs://[HDFS_HOST]:8020/user/victor/mytable'; msck repair table mytable; select * from mytable limit 100; Notes Data should be stored in Snappy Parquet files and respecting the defined partitioning See this example , where a DataFrame writes compressed data.","title":"Hive"},{"location":"bigdata/hive/#hive-ddl-manual","text":"Link","title":"Hive DDL Manual"},{"location":"bigdata/hive/#hive-tables-over-snappy-parquet","text":"Useful script for creating partitioned, snappy-based, Hive tables. drop table mytable; CREATE EXTERNAL TABLE `mytable`( `mytimestamp` string, `key` string, `value` double, PARTITIONED BY ( `year` int, `month` int, `day` int) STORED AS PARQUET TBLPROPERTIES (\"parquet.compression\"=\"SNAPPY\"); alter table mytable set location 'hdfs://[HDFS_HOST]:8020/user/victor/mytable'; msck repair table mytable; select * from mytable limit 100; Notes Data should be stored in Snappy Parquet files and respecting the defined partitioning See this example , where a DataFrame writes compressed data.","title":"Hive tables over Snappy Parquet"},{"location":"bigdata/metacat/","text":"Metacat https://medium.com/netflix-techblog/metacat-making-big-data-discoverable-and-meaningful-at-netflix-56fb36a53520","title":"Metacat"},{"location":"bigdata/orc/","text":"ORC Support TODO","title":"Orc"},{"location":"bigdata/parquet/","text":"Manual Parquet generation with Kite SDK Installation: curl http://central.maven.org/maven2/org/kitesdk/kite-tools/1.1.0/kite-tools-1.1.0-binary.jar -o kite-dataset chmod +x kite-dataset Create the files\u00b4 schema: { \"type\" : \"record\", \"name\" : \"GenericCell4gHdfs\", \"namespace\" : \"com.mydomain.mypojo\", \"doc\" : \"Sample object\", \"fields\" : [ { \"name\" : \"timestamp\", \"type\" : \"string\" }, { \"name\" : \"value\", \"type\" : [ \"null\", \"double\" ], \"default\" : null }, { \"name\" : \"longValue\", \"type\" : \"long\" }, { \"name\" : \"intValue\", \"type\" : \"int\" } ] } Create the partitions`definition: With this JSON: [ { \"name\" : \"year\", \"source\" : \"timestamp\", \"type\" : \"year\" }, { \"name\" : \"month\", \"source\" : \"timestamp\", \"type\" : \"month\" }, { \"name\" : \"day\", \"source\" : \"timestamp\", \"type\" : \"day\" } ] And then invoke: kite-dataset partition-config timestamp:year timestamp:month timestamp:day -s schema.txt -o partitions.txt Create the dataset kite-dataset create dataset:hdfs:/victor/mypojo --schema edos_schema.txt --format parquet -p partitions.txt Finally, Import the data from a CSV kite-dataset csv-import bigdata.csv dataset:hdfs:/victor/mypojo --use-hdfs","title":"Parquet"},{"location":"bigdata/parquet/#manual-parquet-generation-with-kite-sdk","text":"Installation: curl http://central.maven.org/maven2/org/kitesdk/kite-tools/1.1.0/kite-tools-1.1.0-binary.jar -o kite-dataset chmod +x kite-dataset Create the files\u00b4 schema: { \"type\" : \"record\", \"name\" : \"GenericCell4gHdfs\", \"namespace\" : \"com.mydomain.mypojo\", \"doc\" : \"Sample object\", \"fields\" : [ { \"name\" : \"timestamp\", \"type\" : \"string\" }, { \"name\" : \"value\", \"type\" : [ \"null\", \"double\" ], \"default\" : null }, { \"name\" : \"longValue\", \"type\" : \"long\" }, { \"name\" : \"intValue\", \"type\" : \"int\" } ] } Create the partitions`definition: With this JSON: [ { \"name\" : \"year\", \"source\" : \"timestamp\", \"type\" : \"year\" }, { \"name\" : \"month\", \"source\" : \"timestamp\", \"type\" : \"month\" }, { \"name\" : \"day\", \"source\" : \"timestamp\", \"type\" : \"day\" } ] And then invoke: kite-dataset partition-config timestamp:year timestamp:month timestamp:day -s schema.txt -o partitions.txt Create the dataset kite-dataset create dataset:hdfs:/victor/mypojo --schema edos_schema.txt --format parquet -p partitions.txt Finally, Import the data from a CSV kite-dataset csv-import bigdata.csv dataset:hdfs:/victor/mypojo --use-hdfs","title":"Manual Parquet generation with Kite SDK"},{"location":"docker/cloudera/","text":"Cloudera Quickstart docker container Pull the image # Pulls the image (4Gb!) docker pull cloudera/quickstart Set a variable with all needed ports to be opened # Ports to be opened (most commonly used) # - 8888 expose hue interface # - 7180 expose cloudera manager # - 80 expose cloudera examples # - 8983 expose port of Web UI solr search # - 50070 expose name node web ui interface # - 50090 expose secondary name node # - 50075 expose data node # - 50030 expose job tracker # - 50060 expose task trackers # - 60010 expose hbase master status # - 60030 expose hbase region server # - 9095 expose hbase thrift server # - 8020 expose hdfs port # - 8088 expose job tracker port # - 4040 expose port of spark # - 18088 expose history server web interface # ... ports=\"-p 8888:8888 -p 7180:7180 -p 80:80 -p 4040:4040 -p 4041:4041 \\ -p 4042:4042 -p 4043:4043 -p 9092:9092 -p 2181:2181 -p 8020:8020 \\ -p 18088:18088 -p 10000:10000 -p 21050:21050 -p 50070:50070 -p 50075:50075 \\ -p 50060:50060 -p 50030:50030 -p 50010:50010\" Set a variable with a volume for file exchange with the container # Volume used to exchange stuff with the running container localVolumeDir=/home/me/cloudera_exchange_dir Run the container and store its ID containerId=`docker run --hostname=quickstart.cloudera -d \\ -v $localVolumeDir:/volume --privileged=true -t -i $ports cloudera/quickstart /usr/bin/docker-quickstart` Install and run the Kafka server Kafka does not seem to be installed, but you can quickly do it: # See https://kafka.apache.org/quickstart # Download the binaries and execute this command: # > bin/kafka-server-start.sh config/server.properties # If you want to access Kafka from outside the container, you need to change this line in server.properties # zookeeper.connect=localhost:2181 # And replace it with the Docker host public IP address Optional: Run Cloudera Manager sudo su cd /home/cloudera/ ./cloudera-manager","title":"Cloudera Quickstart"},{"location":"docker/cloudera/#cloudera-quickstart-docker-container","text":"Pull the image # Pulls the image (4Gb!) docker pull cloudera/quickstart Set a variable with all needed ports to be opened # Ports to be opened (most commonly used) # - 8888 expose hue interface # - 7180 expose cloudera manager # - 80 expose cloudera examples # - 8983 expose port of Web UI solr search # - 50070 expose name node web ui interface # - 50090 expose secondary name node # - 50075 expose data node # - 50030 expose job tracker # - 50060 expose task trackers # - 60010 expose hbase master status # - 60030 expose hbase region server # - 9095 expose hbase thrift server # - 8020 expose hdfs port # - 8088 expose job tracker port # - 4040 expose port of spark # - 18088 expose history server web interface # ... ports=\"-p 8888:8888 -p 7180:7180 -p 80:80 -p 4040:4040 -p 4041:4041 \\ -p 4042:4042 -p 4043:4043 -p 9092:9092 -p 2181:2181 -p 8020:8020 \\ -p 18088:18088 -p 10000:10000 -p 21050:21050 -p 50070:50070 -p 50075:50075 \\ -p 50060:50060 -p 50030:50030 -p 50010:50010\" Set a variable with a volume for file exchange with the container # Volume used to exchange stuff with the running container localVolumeDir=/home/me/cloudera_exchange_dir Run the container and store its ID containerId=`docker run --hostname=quickstart.cloudera -d \\ -v $localVolumeDir:/volume --privileged=true -t -i $ports cloudera/quickstart /usr/bin/docker-quickstart` Install and run the Kafka server Kafka does not seem to be installed, but you can quickly do it: # See https://kafka.apache.org/quickstart # Download the binaries and execute this command: # > bin/kafka-server-start.sh config/server.properties # If you want to access Kafka from outside the container, you need to change this line in server.properties # zookeeper.connect=localhost:2181 # And replace it with the Docker host public IP address Optional: Run Cloudera Manager sudo su cd /home/cloudera/ ./cloudera-manager","title":"Cloudera Quickstart docker container"},{"location":"docker/sqlserver/","text":"SQL Server on Docker General Instructions Related StackOverflow question: How to solve \"2017-GA: The transaction log for database 'master' is full due to 'NOTHING'. #180\" error Command to run the container: sudo docker run -e 'ACCEPT_EULA=Y' -e 'MSSQL_SA_PASSWORD=<YourStrong!Passw0rd>' \\ -p 1401:1433 --name sql1 -v /data/ms-sql-docker-data:/var/opt/mssql \\ -d microsoft/mssql-server-linux:2017-latest","title":"SQL Server"},{"location":"docker/sqlserver/#sql-server-on-docker","text":"General Instructions Related StackOverflow question: How to solve \"2017-GA: The transaction log for database 'master' is full due to 'NOTHING'. #180\" error Command to run the container: sudo docker run -e 'ACCEPT_EULA=Y' -e 'MSSQL_SA_PASSWORD=<YourStrong!Passw0rd>' \\ -p 1401:1433 --name sql1 -v /data/ms-sql-docker-data:/var/opt/mssql \\ -d microsoft/mssql-server-linux:2017-latest","title":"SQL Server on Docker"},{"location":"elasticsearch/geohashing/","text":"Docker commands for creating the environment $ docker pull elasticsearch $ docker pull kibana $ docker run -p 9200:9200 -p 9300:9300 -d elasticsearch $ docker run -p 5601:5601 -d kibana -e http://[ES CONTAINER IP]:9200 Index creation PUT http://[ES CONTAINER IP]:9200/my_locations2 { \"mappings\": { \"location\": { \"properties\": { \"pin\": { \"properties\": { \"location\": { \"type\": \"geo_point\", \"geohash\" : \"true\", // \"geohash_prefix\" : \"true\", // \"geohash_precision\" : 10 } } } } } } } Post for inserting data { \"pin\" : { \"location\" : { \"lat\" : 40.12, \"lon\" : -71.34 } } } Query with geo bounding box curl -X GET \"localhost:9200/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\" : { \"must\" : { \"match_all\" : {} }, \"filter\" : { \"geo_bounding_box\" : { \"pin.location\" : { \"top_left\" : { \"lat\" : 30.29356, \"lon\" : -82.085741 }, \"bottom_right\" : { \"lat\" : 29.661059, \"lon\" : -81.658345 } } } } } } } Bulk inserting object GeoLoader { // Sample data // https://support.spatialkey.com/spatialkey-sample-csv-data/ // Relevant fields from the CSV file val POLICY_ID = 0 val TOTAL_INVESTED = 8 val LATITUDE = 13 val LONGITUDE = 14 def main(args: Array[String]) { val filename = args[0] val elkIP = args[1] println(s\"About to send the file ${filename} to ELK@${elkIP}\") val lines = Source.fromFile(filename).getLines().drop(1) val payload = lines.map(_.split(\",\")).map(x => (x(POLICY_ID),x(TOTAL_INVESTED),x(LATITUDE).toFloat,x(LONGITUDE).toFloat)) val bulkPuts = payload.flatMap(x => Seq(s\"\"\"{ \"create\" : { \"_index\" : \"my_locations2\", \"_type\" : \"location\" , \"_id\" : \"${x._1}\"} }\"\"\", s\"\"\"{\"pin\" : { \"location\" : { \"lat\" : ${x._3}, \"lon\" : ${x._4} }, \"key\" : \"${x._1}\", \"tiv\" : ${x._2}}}\"\"\")) println(Http(s\"http://${elkIP}:9200/_bulk\") .timeout(connTimeoutMs = 1000, readTimeoutMs = 60000) .header(\"content-type\", \"application/json\") .postData(bulkPuts.mkString(\"\\n\")).asString) } }","title":"Geographical"},{"location":"elasticsearch/geohashing/#docker-commands-for-creating-the-environment","text":"$ docker pull elasticsearch $ docker pull kibana $ docker run -p 9200:9200 -p 9300:9300 -d elasticsearch $ docker run -p 5601:5601 -d kibana -e http://[ES CONTAINER IP]:9200","title":"Docker commands for creating the environment"},{"location":"elasticsearch/geohashing/#index-creation","text":"PUT http://[ES CONTAINER IP]:9200/my_locations2 { \"mappings\": { \"location\": { \"properties\": { \"pin\": { \"properties\": { \"location\": { \"type\": \"geo_point\", \"geohash\" : \"true\", // \"geohash_prefix\" : \"true\", // \"geohash_precision\" : 10 } } } } } } }","title":"Index creation"},{"location":"elasticsearch/geohashing/#post-for-inserting-data","text":"{ \"pin\" : { \"location\" : { \"lat\" : 40.12, \"lon\" : -71.34 } } }","title":"Post for inserting data"},{"location":"elasticsearch/geohashing/#query-with-geo-bounding-box","text":"curl -X GET \"localhost:9200/_search\" -H 'Content-Type: application/json' -d' { \"query\": { \"bool\" : { \"must\" : { \"match_all\" : {} }, \"filter\" : { \"geo_bounding_box\" : { \"pin.location\" : { \"top_left\" : { \"lat\" : 30.29356, \"lon\" : -82.085741 }, \"bottom_right\" : { \"lat\" : 29.661059, \"lon\" : -81.658345 } } } } } } }","title":"Query with geo bounding box"},{"location":"elasticsearch/geohashing/#bulk-inserting","text":"object GeoLoader { // Sample data // https://support.spatialkey.com/spatialkey-sample-csv-data/ // Relevant fields from the CSV file val POLICY_ID = 0 val TOTAL_INVESTED = 8 val LATITUDE = 13 val LONGITUDE = 14 def main(args: Array[String]) { val filename = args[0] val elkIP = args[1] println(s\"About to send the file ${filename} to ELK@${elkIP}\") val lines = Source.fromFile(filename).getLines().drop(1) val payload = lines.map(_.split(\",\")).map(x => (x(POLICY_ID),x(TOTAL_INVESTED),x(LATITUDE).toFloat,x(LONGITUDE).toFloat)) val bulkPuts = payload.flatMap(x => Seq(s\"\"\"{ \"create\" : { \"_index\" : \"my_locations2\", \"_type\" : \"location\" , \"_id\" : \"${x._1}\"} }\"\"\", s\"\"\"{\"pin\" : { \"location\" : { \"lat\" : ${x._3}, \"lon\" : ${x._4} }, \"key\" : \"${x._1}\", \"tiv\" : ${x._2}}}\"\"\")) println(Http(s\"http://${elkIP}:9200/_bulk\") .timeout(connTimeoutMs = 1000, readTimeoutMs = 60000) .header(\"content-type\", \"application/json\") .postData(bulkPuts.mkString(\"\\n\")).asString) } }","title":"Bulk inserting"},{"location":"git/ssh-clone/","text":"Clone a repo using a SSH private key To clone a repo using a previously registred SSH key ssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git' See this StackOverflow question","title":"SSH Usage"},{"location":"git/ssh-clone/#clone-a-repo-using-a-ssh-private-key","text":"To clone a repo using a previously registred SSH key ssh-agent bash -c 'ssh-add /somewhere/yourkey; git clone git@github.com:user/project.git' See this StackOverflow question","title":"Clone a repo using a SSH private key"},{"location":"haproxy/http2/","text":"How to Enable HTTP2 in HAProxy Source Based on this blog post . Create a certificate This is an easy way to generate a self-signed certificate. Not a recommended production setup! $ openssl req \\ -x509 \\ -sha256 \\ -newkey rsa:4096 \\ -keyout \"test.com.key\" -out \"test.com.pem\" \\ -days 730 \\ -nodes \\ -subj //C=BR/ST=SaoPaulo/L=SaoPaulo/O=TestOrg/OU=TestUnit/CN=test.com Concatenate both private and public keys and pass them to HAProxy cat ./test.com.pem ./test.com.key > haproxy_test.com Dockerfile FROM haproxy COPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg COPY haproxy_test.com /usr/local/etc/haproxy/certs/ COPY 404.http /tmp/sample/errorfiles/ Build and run the Docker container docker build . docker run -d --name haproxy-https -p 443:443 -p 80:80 [IMAGE_ID] Add test.com to the etc/hosts file haproxy.cfg global # Setting the maximum size of the Diffie-hellman parameters # used in the TLS negotiation such that HAProxy won't warn us # about the low default value of 1024. tune.ssl.default-dh-param 2048 defaults # As we're sticking with using HTTP/1.1 all the way # we can set the mode in the `defaults` section and # have it applied everywhere. # # If you set only in the frontend, then haproxy will # complain about letting backend be tcp. mode http frontend https # Binding to port 443 forces us to have higher privileges # when launching HAProxy, but it allows us not to have to # set the `:<port>` in the URL when connecting to it using # a browser (as `https` will imply port 443). # # The certificate set here is the one that contains both the # private key and the actual `.pem` that we generated using # openssl. # # ps.: if you have *a bunch* of certificates to serve, then # you should switch to `crt-list` as HAProxy has a limit on # the size of each line in this config file. bind *:443 ssl crt /usr/local/etc/haproxy/certs/haproxy_test.com alpn h2,http/1.1 # Simple ACL to send traffic to our custom tailored web server. # # This way we can exercise a \"not found\" when not having `HOST` # properly set. # # This also demonstrates how HAProxy is indeed decyphering the # traffic (otherwise it wouldn't catch the Host header). acl host_matches hdr_dom(host) test.com use_backend desired_backend if host_matches default_backend not_found # Our server that serves `/tmp/sample/www` using python's # SimpleHTTPServer module. backend desired_backend server myserver 192.168.1.134:8000 # A dumb \"not-found\" auto-responder using HAProxy's errorfile # directive. # # As the ACL that send traffic to this backend is the default # (and least prioritized), when we reach this backend, it means # that we didn't find a desired backend, thus we should serve # a 404 instead of the generic 503 that haproxy gives back to # clients when something goes wrong. backend not_found errorfile 503 /tmp/sample/errorfiles/404.http","title":"How to Enable HTTP2 in HAProxy"},{"location":"haproxy/http2/#how-to-enable-http2-in-haproxy","text":"Source Based on this blog post . Create a certificate This is an easy way to generate a self-signed certificate. Not a recommended production setup! $ openssl req \\ -x509 \\ -sha256 \\ -newkey rsa:4096 \\ -keyout \"test.com.key\" -out \"test.com.pem\" \\ -days 730 \\ -nodes \\ -subj //C=BR/ST=SaoPaulo/L=SaoPaulo/O=TestOrg/OU=TestUnit/CN=test.com Concatenate both private and public keys and pass them to HAProxy cat ./test.com.pem ./test.com.key > haproxy_test.com Dockerfile FROM haproxy COPY haproxy.cfg /usr/local/etc/haproxy/haproxy.cfg COPY haproxy_test.com /usr/local/etc/haproxy/certs/ COPY 404.http /tmp/sample/errorfiles/ Build and run the Docker container docker build . docker run -d --name haproxy-https -p 443:443 -p 80:80 [IMAGE_ID] Add test.com to the etc/hosts file haproxy.cfg global # Setting the maximum size of the Diffie-hellman parameters # used in the TLS negotiation such that HAProxy won't warn us # about the low default value of 1024. tune.ssl.default-dh-param 2048 defaults # As we're sticking with using HTTP/1.1 all the way # we can set the mode in the `defaults` section and # have it applied everywhere. # # If you set only in the frontend, then haproxy will # complain about letting backend be tcp. mode http frontend https # Binding to port 443 forces us to have higher privileges # when launching HAProxy, but it allows us not to have to # set the `:<port>` in the URL when connecting to it using # a browser (as `https` will imply port 443). # # The certificate set here is the one that contains both the # private key and the actual `.pem` that we generated using # openssl. # # ps.: if you have *a bunch* of certificates to serve, then # you should switch to `crt-list` as HAProxy has a limit on # the size of each line in this config file. bind *:443 ssl crt /usr/local/etc/haproxy/certs/haproxy_test.com alpn h2,http/1.1 # Simple ACL to send traffic to our custom tailored web server. # # This way we can exercise a \"not found\" when not having `HOST` # properly set. # # This also demonstrates how HAProxy is indeed decyphering the # traffic (otherwise it wouldn't catch the Host header). acl host_matches hdr_dom(host) test.com use_backend desired_backend if host_matches default_backend not_found # Our server that serves `/tmp/sample/www` using python's # SimpleHTTPServer module. backend desired_backend server myserver 192.168.1.134:8000 # A dumb \"not-found\" auto-responder using HAProxy's errorfile # directive. # # As the ACL that send traffic to this backend is the default # (and least prioritized), when we reach this backend, it means # that we didn't find a desired backend, thus we should serve # a 404 instead of the generic 503 that haproxy gives back to # clients when something goes wrong. backend not_found errorfile 503 /tmp/sample/errorfiles/404.http","title":"How to Enable HTTP2 in HAProxy"},{"location":"java/crypto/","text":"Use the cryptographic extensions in Java Installation Download from here and copy to the local Java home folder. Create a certificate This is an easy way to generate a self-signed certificate. Not a recommended production setup! $ keytool -genkeypair -alias mytestkey -keyalg RSA \\ -dname \"CN=WebServer,OU=Unit,O=Organization,L=City,S=State,C=US\" \\ -keypass changeme -keystore server.jks -storepass letmein","title":"Cryptography"},{"location":"java/crypto/#use-the-cryptographic-extensions-in-java","text":"Installation Download from here and copy to the local Java home folder. Create a certificate This is an easy way to generate a self-signed certificate. Not a recommended production setup! $ keytool -genkeypair -alias mytestkey -keyalg RSA \\ -dname \"CN=WebServer,OU=Unit,O=Organization,L=City,S=State,C=US\" \\ -keypass changeme -keystore server.jks -storepass letmein","title":"Use the cryptographic extensions in Java"},{"location":"java/jvm/","text":"How to open JMX ports JVM options to be set: -Djavax.management.builder.initial= -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8855 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false See this How to set up JVM memory settings JVM options to be set: # Starts with 16G and set a maximum of 24G -Xms16G -Xmx24G","title":"JVM"},{"location":"java/jvm/#how-to-open-jmx-ports","text":"JVM options to be set: -Djavax.management.builder.initial= -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=8855 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false See this","title":"How to open JMX ports"},{"location":"java/jvm/#how-to-set-up-jvm-memory-settings","text":"JVM options to be set: # Starts with 16G and set a maximum of 24G -Xms16G -Xmx24G","title":"How to set up JVM memory settings"},{"location":"java/properties/","text":"Load a java.util.properties from File/URL/HDFS /** * Utility class that provides logic for reading Properties objects from several locations: * - URL (HTTP/S) * - File * - HDFS */ public class PropertiesReader { private static final Logger LOGGER = LoggerFactory.getLogger(PropertiesReader.class); private static final String HTTP_RESOURCE_REGEX = \"(http[s]?://.*[:[0-9]+]?/.*)\"; /** * Gets an InputStream from either a File, URL or HDFS public static InputStream getFileInputStream(String path) throws IOException { // Try with an HTTP(S) resource Pattern httpPat = Pattern.compile(HTTP_RESOURCE_REGEX); Matcher httpMatcher = httpPat.matcher(path); if (httpMatcher.matches()){ return getHTTPInputStream(httpMatcher.group()); } // TODO Copy HDFS block // Now try with a regular file return new FileInputStream(path); } /** * @param path String representing the location of the properties file. Can be HDFS, URL or local file * @return the loaded Properties object * @throws URISyntaxException * @throws IOException */ public static Properties getProperties(String path) throws IOException { // Try with an HDFS resource Properties toReturn = new Properties(); toReturn.load(getFileInputStream(path)); return toReturn; } private static InputStream getHTTPInputStream(String path) throws IOException { LOGGER.info(String.format(\"Trying to open URL at \" + path)); return new URL(path); } }","title":"File/URL/HDFS Properties"},{"location":"java/properties/#load-a-javautilproperties-from-fileurlhdfs","text":"/** * Utility class that provides logic for reading Properties objects from several locations: * - URL (HTTP/S) * - File * - HDFS */ public class PropertiesReader { private static final Logger LOGGER = LoggerFactory.getLogger(PropertiesReader.class); private static final String HTTP_RESOURCE_REGEX = \"(http[s]?://.*[:[0-9]+]?/.*)\"; /** * Gets an InputStream from either a File, URL or HDFS public static InputStream getFileInputStream(String path) throws IOException { // Try with an HTTP(S) resource Pattern httpPat = Pattern.compile(HTTP_RESOURCE_REGEX); Matcher httpMatcher = httpPat.matcher(path); if (httpMatcher.matches()){ return getHTTPInputStream(httpMatcher.group()); } // TODO Copy HDFS block // Now try with a regular file return new FileInputStream(path); } /** * @param path String representing the location of the properties file. Can be HDFS, URL or local file * @return the loaded Properties object * @throws URISyntaxException * @throws IOException */ public static Properties getProperties(String path) throws IOException { // Try with an HDFS resource Properties toReturn = new Properties(); toReturn.load(getFileInputStream(path)); return toReturn; } private static InputStream getHTTPInputStream(String path) throws IOException { LOGGER.info(String.format(\"Trying to open URL at \" + path)); return new URL(path); } }","title":"Load a java.util.properties from File/URL/HDFS"},{"location":"scala/cheatsheets/","text":"Scala cheatsheets and job questions Journal Dev Scala Interview Questions Scala Interview Questions - Github","title":"Cheatsheets"},{"location":"scala/cheatsheets/#scala-cheatsheets-and-job-questions","text":"Journal Dev Scala Interview Questions Scala Interview Questions - Github","title":"Scala cheatsheets and job questions"},{"location":"scala/json/","text":"Code for parsing a JSON document These auxiliary classes help retrieving the values of the different types of JSON objects: // This common class is the base of all classes parsed from the JSON class CC[T] { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) } // Now, for every type that we might recover, we have a specialized class // M (map) and L (list) will be used to recover embedded objects from the JSON object M extends CC[Map[String, Any]] object L extends CC[List[Any]] object S extends CC[String] object D extends CC[Double] object B extends CC[Boolean] Now, after parsing the JSON structure, we crawl along it and extract the needed fields def parseJSONData(data: String) : List[UseCase] = { // We crawl along the JSON structure, retrieving the columns that will form our table structure val resultList = for { Some(M(json)) <- List(JSON.parseFull(data)) // TODO } yield { // TODO }","title":"Parse a JSON"},{"location":"scala/json/#code-for-parsing-a-json-document","text":"These auxiliary classes help retrieving the values of the different types of JSON objects: // This common class is the base of all classes parsed from the JSON class CC[T] { def unapply(a:Any):Option[T] = Some(a.asInstanceOf[T]) } // Now, for every type that we might recover, we have a specialized class // M (map) and L (list) will be used to recover embedded objects from the JSON object M extends CC[Map[String, Any]] object L extends CC[List[Any]] object S extends CC[String] object D extends CC[Double] object B extends CC[Boolean] Now, after parsing the JSON structure, we crawl along it and extract the needed fields def parseJSONData(data: String) : List[UseCase] = { // We crawl along the JSON structure, retrieving the columns that will form our table structure val resultList = for { Some(M(json)) <- List(JSON.parseFull(data)) // TODO } yield { // TODO }","title":"Code for parsing a JSON document"},{"location":"scala/parallel/","text":"Parallel collections Code creating and using a parallel collection: scala> import scala.collection.parallel._ import scala.collection.parallel._ scala> val pc = mutable.ParArray(1, 2, 3) pc: scala.collection.parallel.mutable.ParArray[Int] = ParArray(1, 2, 3) scala> pc.tasksupport = new ForkJoinTaskSupport(new scala.concurrent.forkjoin.ForkJoinPool(2)) warning: there was one deprecation warning (since 2.12.0); for details, enable `:setting -deprecation' or `:replay -deprecation' pc.tasksupport: scala.collection.parallel.TaskSupport = scala.collection.parallel.ForkJoinTaskSupport@59bdcf98 scala> pc map { _ + 1 }","title":"Parallel Collections"},{"location":"scala/parallel/#parallel-collections","text":"Code creating and using a parallel collection: scala> import scala.collection.parallel._ import scala.collection.parallel._ scala> val pc = mutable.ParArray(1, 2, 3) pc: scala.collection.parallel.mutable.ParArray[Int] = ParArray(1, 2, 3) scala> pc.tasksupport = new ForkJoinTaskSupport(new scala.concurrent.forkjoin.ForkJoinPool(2)) warning: there was one deprecation warning (since 2.12.0); for details, enable `:setting -deprecation' or `:replay -deprecation' pc.tasksupport: scala.collection.parallel.TaskSupport = scala.collection.parallel.ForkJoinTaskSupport@59bdcf98 scala> pc map { _ + 1 }","title":"Parallel collections"},{"location":"spark/spark/","text":"Read and write from/to CSV If Spark Version is older than 2.0, this dependency is needed in SBT; \"com.databricks\"%\"spark-csv_2.10\"%\"1.5.0\" Then load the CSV into a DataFrame: # Need to provide a Schame and a File Path val valdf=sqlContext.read .format(\"com.databricks.spark.csv\") .schema(schema) .option(\"delimiter\",\",\") .option(\"nullValue\",\"\") .option(\"header\",\"true\") .option(\"treatEmptyValuesAsNulls\",\"true\") .load(csv_path) To write the DataFrame to a SINGLE CSV file: // Write all to one partition df.coalesce(1).write.format(\"com.databricks.spark.csv\") .option(\"header\",\"true\") .save(\"mydata.csv\") } Writing a JSON DataFrame to Kafka Writing a RDD or DataFrame to Kafka requires the Producers to be created in each RDD partition private def writeJSONToKafka(df : DataFrame, topic: String): Unit = { val props = new util.HashMap[String, Object]() props.put(\"bootstrap.servers\",\"localhost:9092\") props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") println(\"Writing JSON to Kafka\") df.toJSON.foreachPartition((partisions: Iterator[String]) => { val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props) partisions.foreach((line: String) => { producer.send(new ProducerRecord[String, String](topic,null,line)) }) }) } Spark Streaming setup SBT dependencies to be set: libraryDependencies ++= { val sparkVer = \"2.0.0\" Seq( \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-streaming-kafka-0-8\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(), \"org.apache.spark\" %% \"spark-hive\" % sparkVer % \"provided\" ) } Stream set up and processing: // Initialize spark context val conf = new SparkConf().setAppName(\"My Streaming Processor\") val ssc = new StreamingContext(conf, Seconds(30)) ssc.sparkContext.setLogLevel(\"ERROR\") // Kafka config val kafkaConf = Map( // TODO Maybe this one is all we need \"metadata.broker.list\" -> \"localhost:9092\", ) // Create a stream of Array[Byte] as key and as payload val stream = KafkaUtils.createDirectStream[Array[Byte], Array[Byte], DefaultDecoder, DefaultDecoder]( ssc, kafkaConf, Set(\"myTopic\") ) // Process each batch of data in another function stream.foreachRDD { rdd => processRdd(rdd,config)} ssc.start() ssc.awaitTermination() Write a compressed DataFrame val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate() spark.sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\") // Create the DF and add the partition columns val targetDF = spark.sqlContext.createDataFrame(rowData, addPartitions(myTableSchema,partitionsArray)) // Write to Parquet val parquetPath = \"/user/victor/mytable\" targetDF.write.mode(\"append\").partitionBy(partitions : _*).parquet(parquetPath) Add dynamic partitions to a DataFrame Adds dynamic partitions to an existing schema, so they can be given by configuration def addPartitions(schema: StructType, partitions: Array[String]) : StructType = { var array = new ArrayBuffer[StructField]() // Add previous fields schema.foreach(f => array += f) // Add new partition fields partitions.foreach(p => array += new StructField(p,StringType)) new StructType(array.toArray) } val myTableSchema = StructType(Array(StructField(\"mytimestamp\", StringType, true), StructField(\"key\", StringType, true), StructField(\"value\", DoubleType, true))) Compare two rows within two Dataframes/two Parquet Files // Read the files val df1 = spark.read.parquet(path1) val df2 = spark.read.parquet(path2) // Clean the metadata from the schema val schema_df1 = df1.schema.map(_.copy(metadata = Metadata.empty)) val schema_df2 = df2.schema.map(_.copy(metadata = Metadata.empty)) println(s\"Do schemas match? ${schema_df1 == schema_df2}\") // Get a new dataframe with the rows in the second that dont exist in the first val diff = df1.except(df2) println(s\"${diff.count()} rows found to be different\")","title":"DataFrames"},{"location":"spark/spark/#read-and-write-fromto-csv","text":"If Spark Version is older than 2.0, this dependency is needed in SBT; \"com.databricks\"%\"spark-csv_2.10\"%\"1.5.0\" Then load the CSV into a DataFrame: # Need to provide a Schame and a File Path val valdf=sqlContext.read .format(\"com.databricks.spark.csv\") .schema(schema) .option(\"delimiter\",\",\") .option(\"nullValue\",\"\") .option(\"header\",\"true\") .option(\"treatEmptyValuesAsNulls\",\"true\") .load(csv_path) To write the DataFrame to a SINGLE CSV file: // Write all to one partition df.coalesce(1).write.format(\"com.databricks.spark.csv\") .option(\"header\",\"true\") .save(\"mydata.csv\") }","title":"Read and write from/to CSV"},{"location":"spark/spark/#writing-a-json-dataframe-to-kafka","text":"Writing a RDD or DataFrame to Kafka requires the Producers to be created in each RDD partition private def writeJSONToKafka(df : DataFrame, topic: String): Unit = { val props = new util.HashMap[String, Object]() props.put(\"bootstrap.servers\",\"localhost:9092\") props.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") props.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\") println(\"Writing JSON to Kafka\") df.toJSON.foreachPartition((partisions: Iterator[String]) => { val producer: KafkaProducer[String, String] = new KafkaProducer[String, String](props) partisions.foreach((line: String) => { producer.send(new ProducerRecord[String, String](topic,null,line)) }) }) }","title":"Writing a JSON DataFrame to Kafka"},{"location":"spark/spark/#spark-streaming-setup","text":"SBT dependencies to be set: libraryDependencies ++= { val sparkVer = \"2.0.0\" Seq( \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-streaming-kafka-0-8\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(), \"org.apache.spark\" %% \"spark-hive\" % sparkVer % \"provided\" ) } Stream set up and processing: // Initialize spark context val conf = new SparkConf().setAppName(\"My Streaming Processor\") val ssc = new StreamingContext(conf, Seconds(30)) ssc.sparkContext.setLogLevel(\"ERROR\") // Kafka config val kafkaConf = Map( // TODO Maybe this one is all we need \"metadata.broker.list\" -> \"localhost:9092\", ) // Create a stream of Array[Byte] as key and as payload val stream = KafkaUtils.createDirectStream[Array[Byte], Array[Byte], DefaultDecoder, DefaultDecoder]( ssc, kafkaConf, Set(\"myTopic\") ) // Process each batch of data in another function stream.foreachRDD { rdd => processRdd(rdd,config)} ssc.start() ssc.awaitTermination()","title":"Spark Streaming setup"},{"location":"spark/spark/#write-a-compressed-dataframe","text":"val spark = SparkSession.builder.config(rdd.sparkContext.getConf).getOrCreate() spark.sqlContext.setConf(\"spark.sql.parquet.compression.codec\", \"snappy\") // Create the DF and add the partition columns val targetDF = spark.sqlContext.createDataFrame(rowData, addPartitions(myTableSchema,partitionsArray)) // Write to Parquet val parquetPath = \"/user/victor/mytable\" targetDF.write.mode(\"append\").partitionBy(partitions : _*).parquet(parquetPath)","title":"Write a compressed DataFrame"},{"location":"spark/spark/#add-dynamic-partitions-to-a-dataframe","text":"Adds dynamic partitions to an existing schema, so they can be given by configuration def addPartitions(schema: StructType, partitions: Array[String]) : StructType = { var array = new ArrayBuffer[StructField]() // Add previous fields schema.foreach(f => array += f) // Add new partition fields partitions.foreach(p => array += new StructField(p,StringType)) new StructType(array.toArray) } val myTableSchema = StructType(Array(StructField(\"mytimestamp\", StringType, true), StructField(\"key\", StringType, true), StructField(\"value\", DoubleType, true)))","title":"Add dynamic partitions to a DataFrame"},{"location":"spark/spark/#compare-two-rows-within-two-dataframestwo-parquet-files","text":"// Read the files val df1 = spark.read.parquet(path1) val df2 = spark.read.parquet(path2) // Clean the metadata from the schema val schema_df1 = df1.schema.map(_.copy(metadata = Metadata.empty)) val schema_df2 = df2.schema.map(_.copy(metadata = Metadata.empty)) println(s\"Do schemas match? ${schema_df1 == schema_df2}\") // Get a new dataframe with the rows in the second that dont exist in the first val diff = df1.except(df2) println(s\"${diff.count()} rows found to be different\")","title":"Compare two rows within two Dataframes/two Parquet Files"},{"location":"spark/structured_streaming/","text":"Structured Streaming Example Producer process used for testing netstat -c | grep tcp | kafka-console-producer --topic topic2345 --broker-list [KAFKA HOST]:9092 Libraries and command line hacks used: export SPARK_KAFKA_VERSION=0.10 (see this ) SBT dependencies libraryDependencies ++= { val sparkVer = \"2.1.0\" Seq( \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(), \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-sql\" % sparkVer % \"provided\", \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % sparkVer, \"org.apache.kafka\" % \"kafka-clients\" % \"0.10.0.1\" ) } Spark Submit command export SPARK_KAFKA_VERSION=0.10 && spark2-submit --class com.ericsson.streaming.structured.StructuredStreamingMain --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.kafka:kafka-clients:0.10.0.1 sparkstructuredstreamingconsumer_2.11-0.1.jar topic2345 172.29.45.149:9092 WARN GitHub Book on Spark Structured Streaming","title":"Structured Streaming"},{"location":"spark/structured_streaming/#structured-streaming-example","text":"Producer process used for testing netstat -c | grep tcp | kafka-console-producer --topic topic2345 --broker-list [KAFKA HOST]:9092 Libraries and command line hacks used: export SPARK_KAFKA_VERSION=0.10 (see this ) SBT dependencies libraryDependencies ++= { val sparkVer = \"2.1.0\" Seq( \"org.apache.spark\" %% \"spark-core\" % sparkVer % \"provided\" withSources(), \"org.apache.spark\" %% \"spark-streaming\" % sparkVer % \"provided\", \"org.apache.spark\" %% \"spark-sql\" % sparkVer % \"provided\", \"org.apache.spark\" % \"spark-sql-kafka-0-10_2.11\" % sparkVer, \"org.apache.kafka\" % \"kafka-clients\" % \"0.10.0.1\" ) } Spark Submit command export SPARK_KAFKA_VERSION=0.10 && spark2-submit --class com.ericsson.streaming.structured.StructuredStreamingMain --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0,org.apache.kafka:kafka-clients:0.10.0.1 sparkstructuredstreamingconsumer_2.11-0.1.jar topic2345 172.29.45.149:9092 WARN GitHub Book on Spark Structured Streaming","title":"Structured Streaming Example"},{"location":"spring/batch/","text":"Spring Batch: Use seconday datasource for metadata On top of this example , add this @Configuration class: import org.springframework.batch.core.configuration.annotation.DefaultBatchConfigurer; // Another @Configuration class... @Autowired @Qualifier(\"jobsDataSource\") private DataSource dataSource; @Bean public BatchConfigurer configurer() { // This is required to avoid problems when jobs datasource is into some secondary datasource. return new DefaultBatchConfigurer(dataSource); }","title":"Batch"},{"location":"spring/batch/#spring-batch-use-seconday-datasource-for-metadata","text":"On top of this example , add this @Configuration class: import org.springframework.batch.core.configuration.annotation.DefaultBatchConfigurer; // Another @Configuration class... @Autowired @Qualifier(\"jobsDataSource\") private DataSource dataSource; @Bean public BatchConfigurer configurer() { // This is required to avoid problems when jobs datasource is into some secondary datasource. return new DefaultBatchConfigurer(dataSource); }","title":"Spring Batch: Use seconday datasource for metadata"},{"location":"spring/cloud/","text":"Add encryption to a Spring Cloud Config See here on how to setup a certificate. This section describes how to activate encryption in a Spring Cloud Config server. New properties to use # This can be an external location instead! # Given by an administration during deployment time encrypt.keyStore.location=classpath:/server.jks # These properties can be provided as environment variables (more secure!) encrypt.keyStore.password=letmein encrypt.keyStore.alias=mytestkey encrypt.keyStore.secret=changeme Encryption endpoint for administrator usage This endpoints allows encryption/decryption of single properties: # Decryption curl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/decrypt -d [ENCRYPTED_TEXT] # Encryption curl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/encrypt -d myPassword","title":"Cloud"},{"location":"spring/cloud/#add-encryption-to-a-spring-cloud-config","text":"See here on how to setup a certificate. This section describes how to activate encryption in a Spring Cloud Config server. New properties to use # This can be an external location instead! # Given by an administration during deployment time encrypt.keyStore.location=classpath:/server.jks # These properties can be provided as environment variables (more secure!) encrypt.keyStore.password=letmein encrypt.keyStore.alias=mytestkey encrypt.keyStore.secret=changeme Encryption endpoint for administrator usage This endpoints allows encryption/decryption of single properties: # Decryption curl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/decrypt -d [ENCRYPTED_TEXT] # Encryption curl -s -X POST http://xxx.xxx.xxx.xxx:8090/admin/config/encrypt -d myPassword","title":"Add encryption to a Spring Cloud Config"},{"location":"spring/core/","text":"Use two different DataSources To use more than the predefined DataSource in Spring, for instance an H2 Database: ### Add this new set of properties ### ## URL used to connect to the H2 Database spring.secondDatasource.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE ## Driver class used to connect to the H2 database (it will depend on datasource). spring.secondDatasource.driver-class-Name=org.h2.Driver ## User name spring.secondDatasource.username=xxx ## Password spring.secondDatasource.password=xxx ## Datasource configuration for jobs database. spring.jpa.hibernate.ddl-auto=create-drop spring.secondDatasource.initialize=true spring.secondDatasource.test-on-borrow=true spring.secondDatasource.validation-query=select 1 Add this @Configuration class: /** * Config class holding several datasources, the default one and the H2 */ @Configuration public class DataSourceConfiguration { @Bean @Qualifier(\"businessDataSource\") @ConfigurationProperties(prefix = \"spring.datasource\") public DataSource primaryDataSource() { return DataSourceBuilder.create().build(); } @Bean @Primary @Qualifier(\"jobsDataSource\") @ConfigurationProperties(prefix = \"spring.secondDatasource\") public DataSource secondaryDataSource() { return DataSourceBuilder.create().build(); } }","title":"Datasources"},{"location":"spring/core/#use-two-different-datasources","text":"To use more than the predefined DataSource in Spring, for instance an H2 Database: ### Add this new set of properties ### ## URL used to connect to the H2 Database spring.secondDatasource.url=jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;DB_CLOSE_ON_EXIT=FALSE ## Driver class used to connect to the H2 database (it will depend on datasource). spring.secondDatasource.driver-class-Name=org.h2.Driver ## User name spring.secondDatasource.username=xxx ## Password spring.secondDatasource.password=xxx ## Datasource configuration for jobs database. spring.jpa.hibernate.ddl-auto=create-drop spring.secondDatasource.initialize=true spring.secondDatasource.test-on-borrow=true spring.secondDatasource.validation-query=select 1 Add this @Configuration class: /** * Config class holding several datasources, the default one and the H2 */ @Configuration public class DataSourceConfiguration { @Bean @Qualifier(\"businessDataSource\") @ConfigurationProperties(prefix = \"spring.datasource\") public DataSource primaryDataSource() { return DataSourceBuilder.create().build(); } @Bean @Primary @Qualifier(\"jobsDataSource\") @ConfigurationProperties(prefix = \"spring.secondDatasource\") public DataSource secondaryDataSource() { return DataSourceBuilder.create().build(); } }","title":"Use two different DataSources"},{"location":"spring/core_profiles/","text":"How to use dynamic profiles In order to use @Configuration classes with dynamic profiles, one option is to negate the profiles in which we do not want those classes to be activated. However, there is an issue with @Profile(\"!dev\") and the rest of the annotations are still picked up (like @EnableXXX ) To overcome this issue, this @Condition can be used: public abstract class ProfileCondition extends SpringBootCondition { /** * Once implemented, this method will have to assess whether this {@link ProfileCondition} is met give the provided {@link Profile} * @param environment * @return */ protected abstract boolean matchProfiles(final Environment environment); @Override public ConditionOutcome getMatchOutcome(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { if (matchProfiles(conditionContext.getEnvironment())) { return ConditionOutcome.match(\"A local profile has been found.\"); } return ConditionOutcome.noMatch(\"No local profiles found.\"); } } This implementation matches a production setup (no dev and no test ): public class ProdProfileCondition extends ProfileCondition { protected boolean matchProfiles(final Environment environment) { return Arrays.stream(environment.getActiveProfiles()).anyMatch(prof -> { return !prof.equals(\"dev\") && !prof.equals(\"test\"); }); } } Finally, in order to activate it: @Configuration @Profile({\"prod\"}) @Conditional(value = {ProdProfileCondition.class}) @EnableRedisHttpSession public class SecurityConfiguration extends WebSecurityConfigurerAdapter {","title":"Profiles"},{"location":"spring/core_profiles/#how-to-use-dynamic-profiles","text":"In order to use @Configuration classes with dynamic profiles, one option is to negate the profiles in which we do not want those classes to be activated. However, there is an issue with @Profile(\"!dev\") and the rest of the annotations are still picked up (like @EnableXXX ) To overcome this issue, this @Condition can be used: public abstract class ProfileCondition extends SpringBootCondition { /** * Once implemented, this method will have to assess whether this {@link ProfileCondition} is met give the provided {@link Profile} * @param environment * @return */ protected abstract boolean matchProfiles(final Environment environment); @Override public ConditionOutcome getMatchOutcome(ConditionContext conditionContext, AnnotatedTypeMetadata annotatedTypeMetadata) { if (matchProfiles(conditionContext.getEnvironment())) { return ConditionOutcome.match(\"A local profile has been found.\"); } return ConditionOutcome.noMatch(\"No local profiles found.\"); } } This implementation matches a production setup (no dev and no test ): public class ProdProfileCondition extends ProfileCondition { protected boolean matchProfiles(final Environment environment) { return Arrays.stream(environment.getActiveProfiles()).anyMatch(prof -> { return !prof.equals(\"dev\") && !prof.equals(\"test\"); }); } } Finally, in order to activate it: @Configuration @Profile({\"prod\"}) @Conditional(value = {ProdProfileCondition.class}) @EnableRedisHttpSession public class SecurityConfiguration extends WebSecurityConfigurerAdapter {","title":"How to use dynamic profiles"},{"location":"spring/core_ssl/","text":"Built-in HTTPS support Spring-Boot has a built-in support for HTTPS that can be enabled/disabled via settings: server: port: 8446 # SSL settings ssl: enabled: true key-store-type: ${SERVER_KEYSTORE_TYPE:PKCS12} key-store: ${SERVER_KEYSTORE_PATH:classpath:test_cert.p12} key-store-password: ${SERVER_KEYSTORE_PASSWORD:letmein} key-alias: ${SERVER_KEY_ALIAS:test_cert} Note: Eureka settings must be updated as well. eureka: instance: # This needs to be disable when using HTTPS nonSecurePortEnabled: false # This needs to be enabled when using HTTPS securePortEnabled: true # Thess settings needs to reflect HTTPS in the URL + the port used homePageUrl: https://${eureka.hostname}:8446/ statusPageUrl: https://${eureka.hostname}:8446/info Using HTTPS with RestTemplate Once we are exposing our endpoint through HTTPS, we need to adapt the calls made to them. Spring Boot RESTTemplate is easily configurable to use HTTPS when invoking an endpoint: // Build a SSLConnectionSocketFactory with the certificate details SSLConnectionSocketFactory socketFactory = new SSLConnectionSocketFactory( new SSLContextBuilder() // Currently it seems that we can work with self-signed certificates // for inter-service communication .loadTrustMaterial(null, new TrustSelfSignedStrategy()) .loadKeyMaterial(keyStore(ssl), ssl.getKeyStorePassword().toCharArray()) .build(), NoopHostnameVerifier.INSTANCE); // Create the RESTTemplate with the previous SSLConnectionSocketFactory HttpClient httpClient = HttpClients.custom().setSSLSocketFactory(socketFactory).build(); ClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(httpClient); RestTemplate restTemplate = new RestTemplate(requestFactory); This method is used to load the KeyStore details (taken from the configuration properties) private KeyStore keyStore(Ssl ssl) throws KeyStoreException, NoSuchAlgorithmException, CertificateException, FileNotFoundException, IOException { KeyStore keyStore = KeyStore.getInstance(ssl.getKeyStoreType()); // FIXME Is this really necessary? Resource keyStoreResource = resourceLoader.getResource(ssl.getKeyStore()); keyStore.load(keyStoreResource.getInputStream(),ssl.getKeyStorePassword().toCharArray()); return keyStore; }","title":"SSL Support"},{"location":"spring/core_ssl/#built-in-https-support","text":"Spring-Boot has a built-in support for HTTPS that can be enabled/disabled via settings: server: port: 8446 # SSL settings ssl: enabled: true key-store-type: ${SERVER_KEYSTORE_TYPE:PKCS12} key-store: ${SERVER_KEYSTORE_PATH:classpath:test_cert.p12} key-store-password: ${SERVER_KEYSTORE_PASSWORD:letmein} key-alias: ${SERVER_KEY_ALIAS:test_cert} Note: Eureka settings must be updated as well. eureka: instance: # This needs to be disable when using HTTPS nonSecurePortEnabled: false # This needs to be enabled when using HTTPS securePortEnabled: true # Thess settings needs to reflect HTTPS in the URL + the port used homePageUrl: https://${eureka.hostname}:8446/ statusPageUrl: https://${eureka.hostname}:8446/info","title":"Built-in HTTPS support"},{"location":"spring/core_ssl/#using-https-with-resttemplate","text":"Once we are exposing our endpoint through HTTPS, we need to adapt the calls made to them. Spring Boot RESTTemplate is easily configurable to use HTTPS when invoking an endpoint: // Build a SSLConnectionSocketFactory with the certificate details SSLConnectionSocketFactory socketFactory = new SSLConnectionSocketFactory( new SSLContextBuilder() // Currently it seems that we can work with self-signed certificates // for inter-service communication .loadTrustMaterial(null, new TrustSelfSignedStrategy()) .loadKeyMaterial(keyStore(ssl), ssl.getKeyStorePassword().toCharArray()) .build(), NoopHostnameVerifier.INSTANCE); // Create the RESTTemplate with the previous SSLConnectionSocketFactory HttpClient httpClient = HttpClients.custom().setSSLSocketFactory(socketFactory).build(); ClientHttpRequestFactory requestFactory = new HttpComponentsClientHttpRequestFactory(httpClient); RestTemplate restTemplate = new RestTemplate(requestFactory); This method is used to load the KeyStore details (taken from the configuration properties) private KeyStore keyStore(Ssl ssl) throws KeyStoreException, NoSuchAlgorithmException, CertificateException, FileNotFoundException, IOException { KeyStore keyStore = KeyStore.getInstance(ssl.getKeyStoreType()); // FIXME Is this really necessary? Resource keyStoreResource = resourceLoader.getResource(ssl.getKeyStore()); keyStore.load(keyStoreResource.getInputStream(),ssl.getKeyStorePassword().toCharArray()); return keyStore; }","title":"Using HTTPS with RestTemplate"},{"location":"spring/integration/","text":"SFTP with Spring Integration DSL TODO","title":"Integration"},{"location":"spring/integration/#sftp-with-spring-integration-dsl","text":"TODO","title":"SFTP with Spring Integration DSL"},{"location":"spring/security_basic/","text":"Simple in-memory authentication with special rules for several endopoints (local only) @Configuration public class SecurityConfig extends WebSecurityConfigurerAdapter { ... @Autowired public void configureGlobal(AuthenticationManagerBuilder auth) throws Exception { auth .inMemoryAuthentication() .withUser(\"user\").password(user_password).roles(\"USER\") .and() .withUser(\"admin\").password(admin_password).authorities(\"ADMIN\"); } @Override protected void configure(HttpSecurity http) throws Exception { http .csrf() .disable() .httpBasic() .and() .authorizeRequests() .antMatchers(\"/admin/config/decrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\") .antMatchers(\"/admin/config/encrypt\").access(\"hasIpAddress('127.0.0.1/32') and hasAuthority('ADMIN')\") .antMatchers(\"/admin/config/**\").authenticated() .antMatchers(\"/**\").permitAll() ; } } To use IPV4 instead of IPV6 use this: -Djava.net.preferIPv4Stack=true","title":"Basic Setup"},{"location":"spring/security_method/","text":"To enable @PreAuthorize requests to check user roles before allowing certain methods to be executed, this setup is needed: This configuration class enables Method Security and looks for @PathVariable annotation to bind parameters: @Configuration @EnableGlobalMethodSecurity(prePostEnabled = true) public class MethodSecurityConfiguration extends GlobalMethodSecurityConfiguration{ @Override protected MethodSecurityExpressionHandler createExpressionHandler() { DefaultMethodSecurityExpressionHandler result = new DefaultMethodSecurityExpressionHandler(); result.setParameterNameDiscoverer(new AnnotationParameterNameDiscoverer(PathVariable.class.getName())); return result; } } Then, in order to use this in the code: @PreAuthorize(\"hasRole('ADMIN_' + #customer )\") @DeleteMapping(path = \"/{customer }\") public void delete( @PathVariable(\"customer\") String customer) { // Business logic code doing something with customer $customer }","title":"Method Security"},{"location":"spring/spring_io_sessions/","text":"Spring IO 2018 Sessions and Workshops Sessions Continous deployment of your application DDD with Spring: Slides & Video DEVX with Cloud Foundry and Kubernetes Flight of the Flux: Reactor execution model Machine learning exposed Micrometer: New Insights into your Spring Boot Application Observability with Spring-based distributed systems REST beyond the obvious Serverless Spring Spring Cloud Apps on AWS Spring Cloud Gateway Spring Kafka Spring Session 2: Slides & Code Testing your Message-Driven application with Spring Time to Graph Up with Spring Data Neo4J Under the hood of reactive data access Workshops Reactive programming with Netty API Spring 5 Security Monitor your Spring Boot Application with Logs, Metrics and Traces: Code & Slides","title":"Spring IO 2018"},{"location":"spring/spring_io_sessions/#spring-io-2018-sessions-and-workshops","text":"","title":"Spring IO 2018 Sessions and Workshops"},{"location":"spring/spring_io_sessions/#sessions","text":"Continous deployment of your application DDD with Spring: Slides & Video DEVX with Cloud Foundry and Kubernetes Flight of the Flux: Reactor execution model Machine learning exposed Micrometer: New Insights into your Spring Boot Application Observability with Spring-based distributed systems REST beyond the obvious Serverless Spring Spring Cloud Apps on AWS Spring Cloud Gateway Spring Kafka Spring Session 2: Slides & Code Testing your Message-Driven application with Spring Time to Graph Up with Spring Data Neo4J Under the hood of reactive data access","title":"Sessions"},{"location":"spring/spring_io_sessions/#workshops","text":"Reactive programming with Netty API Spring 5 Security Monitor your Spring Boot Application with Logs, Metrics and Traces: Code & Slides","title":"Workshops"}]}